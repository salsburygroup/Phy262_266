{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to two important methods in data analysis: histogramming and curve fitting. You will do this first with synthetic data and then with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: 1D histograming with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# Generate random data\n",
    "data = np.random.randn(1000)\n",
    "\n",
    "# Create a histogram\n",
    "# most of these parameters should be obvious except alpha that is the transparency.\n",
    "plt.hist(data, bins=30, alpha=0.5, color='red', edgecolor='black')\n",
    "plt.title('Histogram of Random Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "What type of distribution does this look like? That indicates the type of distribution that the random number generator is picking randomly from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Vary the number of bins and discuss what happens. Add more code blocks to report more variations. You can also customize the transparency and colors as well if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem that the number of bins to use is arbritrary, however, there are actually several statistical rules to decide how many bins to use of varying complexity, which depends on what assumes you care to make about your data, how much effort you want to do and what you want to minimize.\n",
    "The most common rule is spreadsheet is to set the number of bins equal to the square root of the number of data points; there is no good reason behind that choice.\n",
    "The two most common rules and Sturges's formula and Scott's normal reference rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sturges's formula: $$k=\\lceil \\log _{2}n\\rceil +1$$\n",
    "This was derived almost a century ago from considering binomial distributions, which become normal distributions for large n, and is likely the most commonly used, although it can underfit for small n (n<30 usually>) or oversmooth for large n (n>200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sturges rule to calculate the number of bins (we had 1000 points)\n",
    "bins_sturges = int(1 + np.log2(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Using the number of bins from Sturges's formula as the bin variable, plot a new histogram. Discuss how it compares to your previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a new histogram using Sturge's bins\n",
    "# Create a histogram\n",
    "# most of these parameters should be obvious except alpha that is the transparency.\n",
    "plt.hist(data, bins=bins_sturges, alpha=0.5, color='red', edgecolor='black')\n",
    "plt.title('Histogram of Random Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other rule is Scott's rule to calculate the optimal bin width:\n",
    "$$h = \\frac{3.49\\hat{\\sigma}}{\\sqrt[3]{n}},$$\n",
    "This rule was derived ~45 years ago, and is optimal in the sense of the minimzing the mean square error of the probability function for normal distributions. Note it doesn't just use the number of points, but also the standard deviation of the data. We also have to calculate the number of bins from the range of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "sigma_hat = np.std(data, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "h = 3.49 * sigma_hat / n**(1/3)  # Scott's rule formula\n",
    "data_min, data_max = data.min(), data.max()\n",
    "bins_scott= int((data_max - data_min) / h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: Using the number of bins from Scott's rule as the bin variable, plot a new histogram, and of course, discuss how it differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a new histogram using Scott's bins\n",
    "# Create a histogram\n",
    "# most of these parameters should be obvious except alpha that is the transparency.\n",
    "plt.hist(data, bins=bins_scott, alpha=0.5, color='red', edgecolor='black')\n",
    "plt.title('Histogram of Random Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Curve fitting a histogram with a Gaussian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some times we would like an analytic expression and the most common is to fit with a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now define a gaussian function. \n",
    "#A gaussian has two parameters; the mean value, mu and the standard deviation sigma,\n",
    "def gaussian(x, mu, sigma):\n",
    "    return 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (x - mu)**2 / (2 * sigma**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to curve fit a gaussian, we need to save more information from the historgram, in particular an array of the bin heights and bin borders, and we need to normalize the histogram as a gaussian is normalize.\n",
    "We also need to have error measures, but by calculating the uncertaintity in the parameters, and by calculating the integrated mean standard error, i.e., the derivation from the histogram and the gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate histogram\n",
    "bin_heights, bin_borders, _ =plt.hist(data, bins=bins_scott, alpha=0.5,density=True, color='red', edgecolor='black')\n",
    "# the ,_ says to ignore anything else calculate by the histogram function (like shapre info)\n",
    "#density=True, normalizes the histogram\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [np.mean(data), np.std(data)]\n",
    "# Use curve_fit to fit the gaussian function to the bin_centers and bin_heights\n",
    "popt, pcov = scipy.optimize.curve_fit(gaussian, bin_centers, bin_heights,p0=p0)\n",
    "\n",
    "# Plot the fitted Gaussian curve\n",
    "x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 1000)\n",
    "plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='Fitted Gaussian', color='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean (mu): {popt[0]}, Standard Deviation (sigma): {popt[1]}\")\n",
    "# Calculate the IMSE\n",
    "# Use the fitted Gaussian to predict values over the fine grid\n",
    "predicted_density = gaussian(x_interval_for_fit, *popt)\n",
    "\n",
    "# Calculate the actual density using the histogram data\n",
    "# This involves interpolating the bin heights to match the fine grid used for the fit\n",
    "actual_density = np.interp(x_interval_for_fit, bin_centers, bin_heights)\n",
    "\n",
    "# Calculate the squared difference between the actual and predicted densities\n",
    "squared_diff = (actual_density - predicted_density) ** 2\n",
    "\n",
    "# Integrate the squared difference over the range\n",
    "# Approximate the integral by summing the squared differences and multiplying by the step size\n",
    "imse = np.sum(squared_diff) * (x_interval_for_fit[1] - x_interval_for_fit[0])\n",
    "\n",
    "print(f\"Integrated Mean Squared Error (IMSE): {imse}\")\n",
    "\n",
    "\n",
    "# Calculate the standard deviation (sqrt of diagonal elements of the covariance matrix)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(f\"Parameter uncertanities: {perr}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: The distribution this data is drawn from is a normal (aka gaussian) distribution with mean 0 and sigma of 1. Why is the answer slightly different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4: Test the rationale (aka hypothesis ) you developed to answer Question 2. You might need to change how many data points are selected, and then the number of bins, and then redo the plot, uncertanity and error calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Histogramming in 2D. It is common in physics to have multidemensionaldata, and so 2D histogramming is often common to look at the distribution of data characterized by two variables, followed by display via either a heatmap or a contour map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate random numbers from a 2D normal distribution\n",
    "x = np.random.randn(1000)  # 1000 random numbers for x\n",
    "y = np.random.randn(1000)  # 1000 random numbers for y\n",
    "\n",
    "n = len(x)# assumes each data point has x and y\n",
    "sigma_x = np.std(x, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "hx = 3.49 * sigma_x / n**(1/3)  # Scott's rule formula\n",
    "x_min, x_max = x.min(), x.max()\n",
    "bins_scott_x= int((x_max - x_min) / hx)\n",
    "sigma_y = np.std(y, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "hy = 3.49 * sigma_y/ n**(1/3)  # Scott's rule formula\n",
    "y_min, y_max = y.min(), y.max()\n",
    "bins_scott_y= int((y_max - y_min) / hy)\n",
    "# Creating a 2D histogram from the random numbers\n",
    "heatmap, xedges, yedges = np.histogram2d(x, y, bins=[bins_scott_x,bins_scott_y], density=True)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "# Plotting the 2D heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(heatmap.T, extent=extent, origin='lower', cmap='viridis')\n",
    "plt.title('2D Heatmap')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "# Plotting the 2D contour plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(heatmap.T, extent=extent, cmap='viridis')\n",
    "plt.title('2D Contour Plot')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this isn't the best looking 2D gaussian; you can tell by the lack of symmetry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5: Figure out how you might get a better looking 2D gaussian (hint it has to do with the underlying data )and modify the code until you think it looks reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we would like to more rigorously figure out how to see how good our fit is and go let's do a 2D gaussian fit with uncertainity and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random numbers from a 2D normal distribution\n",
    "x = np.random.randn(1000)  # 1000 random numbers for x\n",
    "y = np.random.randn(1000)  # 1000 random numbers for y\n",
    "\n",
    "n = len(x)# assumes each data point has x and y\n",
    "sigma_x = np.std(x, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "hx = 3.49 * sigma_x / n**(1/3)  # Scott's rule formula\n",
    "x_min, x_max = x.min(), x.max()\n",
    "bins_scott_x= int((x_max - x_min) / hx)\n",
    "sigma_y = np.std(y, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "hy = 3.49 * sigma_y/ n**(1/3)  # Scott's rule formula\n",
    "y_min, y_max = y.min(), y.max()\n",
    "bins_scott_y= int((y_max - y_min) / hy)\n",
    "heatmap, xedges, yedges = np.histogram2d(x, y, bins=[bins_scott_x,bins_scott_y], density=True)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "# Assuming x, y, bins_scott_x, bins_scott_y, and heatmap generation remains unchanged.\n",
    "\n",
    "def gaussian_2d(xy, amplitude, x0, y0, sigma_x, sigma_y):\n",
    "    x, y = xy\n",
    "    return amplitude * np.exp(-(((x - x0)**2 / (2 * sigma_x**2)) + ((y - y0)**2 / (2 * sigma_y**2)))).ravel()\n",
    "\n",
    "# Correcting preparation for curve fitting\n",
    "x_centers = (xedges[:-1] + xedges[1:]) / 2\n",
    "y_centers = (yedges[:-1] + yedges[1:]) / 2\n",
    "X, Y = np.meshgrid(x_centers, y_centers)\n",
    "xy = np.vstack([X.ravel(), Y.ravel()])\n",
    "z = heatmap.T.ravel()  # Ensure z is correctly transposed to match xy arrangement\n",
    "\n",
    "# Curve fitting\n",
    "popt, pcov = scipy.optimize.curve_fit(gaussian_2d, xy, z, p0=[1, 0, 0, 1, 1])\n",
    "\n",
    "# Plotting the fitting results\n",
    "Z_fit = gaussian_2d(xy, *popt).reshape(X.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.contourf(heatmap.T, extent=extent, cmap='viridis') \n",
    "plt.title('Original Data Contour')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.contour(X, Y, Z_fit, colors='red')  # Fitted Gaussian contour\n",
    "plt.title('Fitted 2D Gaussian')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.contourf(heatmap.T, extent=extent, cmap='viridis')\n",
    "plt.contour(X, Y, Z_fit, colors='red')  # Superimposed fitted Gaussian contour\n",
    "plt.title('Original Heatmap and Fitted Contour')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label='Density')\n",
    "\n",
    "\n",
    "# Extracting uncertainties of the parameters\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(\"Uncertainties in the fitted parameters:\", perr)\n",
    "\n",
    "# Ensure Z_fit matches heatmap dimensions for ISME calculation\n",
    "# Transpose Z_fit if its shape is the transpose of heatmap's shape\n",
    "isme = np.sum((heatmap - Z_fit.T)**2) / np.size(Z_fit)\n",
    "print(\"Integrated Square Mean Error (ISME):\", isme)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6:Take your changes you had to do in exercise 5 and add the code right above to fit a gaussian, display the fitted gaussian, the contour with the fitted gaussian and the uncertainties and ISME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 6 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Now let's turn to real data! For the rest of this, you will be looking an analysis of a principle-component analysis of the structures from an molecular dynamics simulation of the protein Thrombin. We might get to principle component analysis but all you need to know is that there are five numbers for each structure in the protein simulation, and by looking at histograms we can tell, what are the main structures we have uncovered in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the data. We will read them in as a pandas array like last time, but then convert to a numpy array in order to do everything you do so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '../data/TM_projections.txt'\n",
    "\n",
    "#read as a pandas array\n",
    "df = pd.read_csv(file_path, sep=' ', header=None) #the file is space deliminated\n",
    "\n",
    "# Assign each column to a separate variable\n",
    "data_x1 = df[0]\n",
    "data_x2 = df[1]\n",
    "data_x3 = df[2]\n",
    "data_x4 = df[3]\n",
    "data_x5 = df[4]\n",
    "\n",
    "# If you want the data as a numpy array, you can convert them as follows:\n",
    "data_x1 = df[0].to_numpy()\n",
    "data_x2 = df[1].to_numpy()\n",
    "data_x3 = df[2].to_numpy()\n",
    "data_x4 = df[3].to_numpy()\n",
    "data_x5 = df[4].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7: Calculate the number of bins using Scott's rule, histogram, fit a gaussian, calculate the errors and uncertanities for the first column. (Hint: Part 2 with some of Part 1 with appropriate variable changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data_x1)\n",
    "sigma_x1 = np.std(data_x1, ddof=1)  # Sample standard deviation (ddof=1 for an unbiased estimator)\n",
    "h_x1= 3.49 * sigma_x1 / n**(1/3)  # Scott's rule formula\n",
    "data_x1_min, data_x1_max = data_x1.min(), data_x1.max()\n",
    "bins_scott_x1= int((data_x1_max - data_x1_min) / h_x1)\n",
    "# Calculate histogram\n",
    "bin_heights, bin_borders, _ =plt.hist(data_x1, bins=bins_scott_x1, alpha=0.5,density=True, color='red', edgecolor='black')\n",
    "# the ,_ says to ignore anything else calculate by the histogram function (like shapre info)\n",
    "#density=True, normalizes the histogram\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [np.mean(data_x1), np.std(data_x1)]\n",
    "# Use curve_fit to fit the gaussian function to the bin_centers and bin_heights\n",
    "popt, pcov = scipy.optimize.curve_fit(gaussian, bin_centers, bin_heights,p0=p0)\n",
    "\n",
    "# Plot the fitted Gaussian curve\n",
    "x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 1000)\n",
    "plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='Fitted Gaussian', color='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean (mu): {popt[0]}, Standard Deviation (sigma): {popt[1]}\")\n",
    "# Calculate the IMSE\n",
    "# Use the fitted Gaussian to predict values over the fine grid\n",
    "predicted_density = gaussian(x_interval_for_fit, *popt)\n",
    "\n",
    "# Calculate the actual density using the histogram data\n",
    "# This involves interpolating the bin heights to match the fine grid used for the fit\n",
    "actual_density = np.interp(x_interval_for_fit, bin_centers, bin_heights)\n",
    "\n",
    "# Calculate the squared difference between the actual and predicted densities\n",
    "squared_diff = (actual_density - predicted_density) ** 2\n",
    "\n",
    "# Integrate the squared difference over the range\n",
    "# Approximate the integral by summing the squared differences and multiplying by the step size\n",
    "imse = np.sum(squared_diff) * (x_interval_for_fit[1] - x_interval_for_fit[0])\n",
    "\n",
    "print(f\"Integrated Mean Squared Error (IMSE): {imse}\")\n",
    "\n",
    "# Calculate the standard deviation (sqrt of diagonal elements of the covariance matrix)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(f\"Parameter uncertanities: {perr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Does this look like it would be well-represented by a single gaussian? Note, a major point of this type of work is to identify different conformational states. Given this, does the figure seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might make sense to see if multiple gaussians might fit. Well let's try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define a multi-gaussian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian(x, *params):\n",
    "    y = np.zeros_like(x)\n",
    "    for i in range(0, len(params), 3):\n",
    "        amplitude = params[i]\n",
    "        mean = params[i+1]\n",
    "        sigma = params[i+2]\n",
    "        y += amplitude * np.exp(-((x - mean)**2) / (2 * sigma**2))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a multi-Gaussian fit. you have to put after num_gaussians how many to test, and also make a guess at the the amplitude, center and standard derivation of each gaussian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8: Try 2 with guesses from the histogram of the amplitude, center, standard deviation for each gaussian. Bad guesses mean the optimization might not converge. What happens? Does the fit improve? If you want to, you can explore 3 gaussians or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input for the number of Gaussians\n",
    "num_gaussians = 2 \n",
    "\n",
    "\n",
    "# Adjust initial guesses based on the number of Gaussians\n",
    "# for each gaussian the initial guess of the amplitude, center and standard dev, seperated by commas\n",
    "p0 = []# initial guesses\n",
    "\n",
    "# Fit the multi-Gaussian model to the data\n",
    "popt, pcov = scipy.optimize.curve_fit(multi_gaussian, bin_centers, bin_heights, p0=p0, maxfev=10000)\n",
    "\n",
    "# Plotting\n",
    "plt.hist(data_x1, bins=bins_scott_x1, alpha=0.5, density=True, color='red', edgecolor='black')\n",
    "x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 1000)\n",
    "plt.plot(x_interval_for_fit, multi_gaussian(x_interval_for_fit, *popt), label='Fitted Multi-Gaussian', color='r')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print parameters for each Gaussian\n",
    "for i in range(num_gaussians):\n",
    "    print(f\"Gaussian {i+1} - Mean (mu): {popt[i*3+1]}, Standard Deviation (sigma): {popt[i*3+2]}\")\n",
    "\n",
    "# IMSE Calculation remains the same as in your script\n",
    "predicted_density = multi_gaussian(x_interval_for_fit, *popt)\n",
    "squared_diff = (actual_density - predicted_density) ** 2\n",
    "imse = np.sum(squared_diff) * (x_interval_for_fit[1] - x_interval_for_fit[0])\n",
    "print(f\"Integrated Mean Squared Error (IMSE): {imse}\")\n",
    "\n",
    "#Calculating parameter uncertanities\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "parameter_names = ['Amplitude', 'Mean', 'Standard Deviation']\n",
    "\n",
    "for i in range(num_gaussians):\n",
    "    print(f\"Gaussian {i+1}:\")\n",
    "    for j, name in enumerate(parameter_names):\n",
    "        param_index = i * len(parameter_names) + j  # Calculate the index of the parameter in the flat list\n",
    "        print(f\"  {name} uncertainty: {perr[param_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all this for the first column of data. That is the first principle component, which is the most important. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9: Plot a histogram with a single gaussian fit for the 2nd and 5th principle components, that is the second and fifth data columns we read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8; column 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8; column 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try and fit the second and fifth components to multigaussian distributions, but it actually isn't that easy as the second potential gaussian just doesn't have very much data in it, so the fitting requires more finesse than we have in this class. Though feel free to experiment on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now discussed that this data -- from a simulation of thrombin where each data point represents a specific 3D structure -- is not well-represented by a single gaussian in 1D, but what happens in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 10: Do a 2D histogram of the first and second principle components, and display as a contour map. Hint: The first part of part 3. Discuss the result probability distribution, refer back to the 1D probability distributions. Does the 2D histograming provide a different view on the data than just looking at the 1D distributions? Discuss of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code for Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious as to what this physically represents, look at the image Phy266_structure.png in the data directory. It depicts a superposition of representative structures from the three highest probability locations on the 2D histogram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
